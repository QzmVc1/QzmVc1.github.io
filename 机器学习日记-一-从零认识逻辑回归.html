<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png">
  <link rel="mask-icon" href="/images/favicon.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CRoboto+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-fill-left.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"qzmvc1.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.8.0","exturl":false,"sidebar":{"position":"left","width":310,"display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"manual"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>


<meta name="description" content="引言 分类技术是机器学习和数据挖掘应用中的重要组成部分。在数据科学中，大约70%的问题属于分类问题。解决分类问题的算法也有很多种，比如：KNN算法，使用距离计算来实现分类；决策树，通过构建直观易懂的树来实现分类；朴素贝叶斯，使用概率论构建分类器。本篇谈到的是Logistic回归，它是一种很常见的用来解决二元分类问题的方法。 逻辑回归(Logistic Regression,简称LR)，虽然它的名字">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习日记(一):从零认识逻辑回归">
<meta property="og:url" content="http://qzmvc1.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0-%E4%B8%80-%E4%BB%8E%E9%9B%B6%E8%AE%A4%E8%AF%86%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html">
<meta property="og:site_name" content="QzmVc1">
<meta property="og:description" content="引言 分类技术是机器学习和数据挖掘应用中的重要组成部分。在数据科学中，大约70%的问题属于分类问题。解决分类问题的算法也有很多种，比如：KNN算法，使用距离计算来实现分类；决策树，通过构建直观易懂的树来实现分类；朴素贝叶斯，使用概率论构建分类器。本篇谈到的是Logistic回归，它是一种很常见的用来解决二元分类问题的方法。 逻辑回归(Logistic Regression,简称LR)，虽然它的名字">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-ab82d755dba95f0585678fe2d4af28d6_hd.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/04/22/EkLNb4.png">
<meta property="article:published_time" content="2019-04-22T07:34:50.000Z">
<meta property="article:modified_time" content="2022-01-31T11:15:20.407Z">
<meta property="article:author" content="QzmVc1">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic3.zhimg.com/80/v2-ab82d755dba95f0585678fe2d4af28d6_hd.jpg">


<link rel="canonical" href="http://qzmvc1.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0-%E4%B8%80-%E4%BB%8E%E9%9B%B6%E8%AE%A4%E8%AF%86%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://qzmvc1.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0-%E4%B8%80-%E4%BB%8E%E9%9B%B6%E8%AE%A4%E8%AF%86%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html","path":"机器学习日记-一-从零认识逻辑回归.html","title":"机器学习日记(一):从零认识逻辑回归"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>

<script>
    (function(){
        if(''){
            if (prompt('加密文，来试试你的运气吧！') !== ''){
                alert('要不要再尝试一次呢~~');
                history.back();
            }
        }
    })();
</script><title>机器学习日记(一):从零认识逻辑回归 | QzmVc1</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">QzmVc1</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Standing on Shoulders of Giants.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-text">引言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92linear-regression"><span class="nav-text">一、线性回归（Linear
Regression）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%A6%82%E4%BD%95%E7%94%A8%E8%BF%9E%E7%BB%AD%E7%9A%84%E6%95%B0%E5%80%BC%E5%8E%BB%E9%A2%84%E6%B5%8B%E7%A6%BB%E6%95%A3%E7%9A%84%E6%A0%87%E7%AD%BE%E5%80%BC"><span class="nav-text">二、如何用连续的数值去预测离散的标签值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92logistics-regression"><span class="nav-text">三、逻辑回归（logistics
regression）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0loss-function"><span class="nav-text">四、逻辑回归的损失函数（Loss
Function）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%94%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1mlemaximum-likelihood-estimation"><span class="nav-text">五、极大似然估计MLE(Maximum
Likelihood Estimation)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%AD%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-text">六、梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%9B%B4%E8%A7%82%E8%A7%A3%E9%87%8A"><span class="nav-text">6.2 梯度下降的直观解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="nav-text">6.3 梯度下降法描述</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%88%E5%86%B3%E6%9D%A1%E4%BB%B6"><span class="nav-text">6.3.1 先决条件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%83%E6%B1%82-fomega-%E7%9A%84%E6%A2%AF%E5%BA%A6-nabla-fomega"><span class="nav-text">七、求 \(F(\omega)\) 的梯度 \(\nabla F(\omega)\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%AB%E6%9B%B4%E6%96%B0%E7%B3%BB%E6%95%B0-omega_i"><span class="nav-text">八、更新系数 \(\omega_i\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B9%9D%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">九、逻辑回归与正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%89%A9%E4%B8%8B%E7%9A%84%E5%B0%B1%E6%98%AF%E5%85%B7%E4%BD%93%E7%9A%84%E6%95%B2%E4%BB%A3%E7%A0%81%E4%BA%86"><span class="nav-text">剩下的就是具体的敲代码了！</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-text">参考链接:</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="QzmVc1"
      src="/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">QzmVc1</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">88</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="mailto:qzmvc1@gmail.com" title="E-Mail → mailto:qzmvc1@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://jcoffeezph.top/" title="https:&#x2F;&#x2F;jcoffeezph.top&#x2F;" rel="noopener" target="_blank">ForMe</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://szd0319.github.io/" title="https:&#x2F;&#x2F;szd0319.github.io&#x2F;" rel="noopener" target="_blank">Silence</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qzmvc1.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0-%E4%B8%80-%E4%BB%8E%E9%9B%B6%E8%AE%A4%E8%AF%86%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang.jpg">
      <meta itemprop="name" content="QzmVc1">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QzmVc1">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习日记(一):从零认识逻辑回归
        </h1>

        <div class="post-meta-container">

          <div class="post-meta">
  
	
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-04-22 15:34:50" itemprop="dateCreated datePublished" datetime="2019-04-22T15:34:50+08:00">2019-04-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-01-31 19:15:20" itemprop="dateModified" datetime="2022-01-31T19:15:20+08:00">2022-01-31</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h3 id="引言">引言</h3>
<p>分类技术是机器学习和数据挖掘应用中的重要组成部分。在数据科学中，大约70%的问题属于分类问题。解决分类问题的算法也有很多种，比如：KNN算法，使用距离计算来实现分类；决策树，通过构建直观易懂的树来实现分类；朴素贝叶斯，使用概率论构建分类器。<strong>本篇谈到的是Logistic回归，它是一种很常见的用来解决二元分类问题的方法。</strong></p>
<p><strong>逻辑回归(Logistic
Regression,简称LR)，虽然它的名字中带有“回归”两个字，但是它最擅长处理的却是分类问题。</strong>
LR分类器适用于各项广义上的分类任务，例如：评论信息的正负情感分析（二分类）、用户点击率（二分类）、用户违约信息预测（二分类）、垃圾邮件检测（二分类）、疾病预测（二分类）、用户等级分类（多分类）等场景。我们这里主要讨论的是二分类问题，<strong>解决了二分类自然就解决了多分类，因为N分类问题可以转化成N个二分类问题。</strong></p>
<span id="more"></span>
<hr />
<h3 id="一线性回归linear-regression">一、线性回归（Linear
Regression）</h3>
线性回归的表达式：
<p>
<span class="math display">\[ f(\bf{x})=\pmb{\omega^{T}x}+b\]</span>
</p>
<p>线性回归对于给定的输入 <span
class="math inline">\(x\)</span>，输出的是一个数值 <span
class="math inline">\(y\)</span>，因此它是一个解决回归问题的模型。</p>
<p>为了消除掉后面的常数项b，我们可以令 <span
class="math inline">\(x^{&#39;}=[1\ \ x]^{T}\)</span>，同时 <span
class="math inline">\(\omega^{&#39;}=[b\ \ w]^T\)</span>
，直线方程可以化简成为：</p>
<p>
<span
class="math display">\[f(\bf{x^{&#39;}})=\pmb{\omega^{&#39;}x^{&#39;}}\]</span>
</p>
<p>在接下来的文章中为了方便，我们所使用的 <span
class="math inline">\(\omega,x\)</span> 其实指代的是 <span
class="math inline">\(\omega^{&#39;},x^{&#39;}\)</span>。</p>
<h3
id="二如何用连续的数值去预测离散的标签值">二、如何用连续的数值去预测离散的标签值</h3>
<p>线性回归的输出是一个数值，而不是一个标签，显然不能直接解决二分类问题。那我如何改进我们的回归模型来预测标签呢？</p>
<p>一个最直观的办法就是设定一个阈值，比如0。如果我们预测的数值 y&gt;0
，那么属于标签A，反之属于标签B，采用这种方法的模型又叫做<strong>感知机（Perceptron）</strong>。</p>
<p>另一种方法，我们不去直接预测标签，而是去预测标签为A概率，我们知道概率是一个[0,1]区间的连续数值，那我们的输出的数值就是标签为A的概率。一般的如果标签为A的概率大于0.5，我们就认为它是A类，否则就是B类。这就是我们的这次的主角<strong>逻辑回归模型
(Logistics Regression)</strong>。</p>
<h3 id="三逻辑回归logistics-regression">三、逻辑回归（logistics
regression）</h3>
<p>通过以上分析，我们明确了预测目标是标签为A的概率。</p>
<p>我们知道，概率是属于[0,1]区间。但是线性模型 <span
class="math inline">\(f(x) = \omega^Tx\)</span> 值域是 <span
class="math inline">\((-\infty,\infty)\)</span>。</p>
<p>我们不能直接基于线性模型建模。我们需要找到一个模型的值域刚好在[0,1]区间，同时要足够好用。</p>
于是，选择了我们的<strong>sigmoid函数</strong>：
<p>
<span class="math display">\[\sigma(x)=\frac{1}{1+e^{-x}}\]</span>
</p>
<p>函数图像为：</p>
<center>
<img src="https://pic3.zhimg.com/80/v2-ab82d755dba95f0585678fe2d4af28d6_hd.jpg">
</center>
<p>但是我们不能直接拿sigmoid函数就用，毕竟它连要训练的参数 <span
class="math inline">\(\omega\)</span> 都没有。</p>
<p>因此我们结合sigmoid函数，线性回归函数，把线性回归模型的输出作为sigmoid函数的输入。于是最后就变成了逻辑回归模型：</p>
<p>
<span
class="math display">\[y=\sigma(f(x))=\sigma(\omega^Tx)=\frac{1}{1+e^{-\pmb{\omega^Tx}}}\]</span>
</p>
<p>假设我们已经训练好了一组权值 <span
class="math inline">\(\omega^T\)</span>。只要把我们需要预测的 <span
class="math inline">\(x\)</span>
代入到上面的方程，输出的y值就是这个标签为A的概率，我们就能够判断输入数据是属于哪个类别。</p>
<p>接下来就来详细介绍，如何利用一组采集到的真实样本，训练出参数 <span
class="math inline">\(\pmb{\omega}\)</span> 的值。</p>
<h3 id="四逻辑回归的损失函数loss-function">四、逻辑回归的损失函数（Loss
Function）</h3>
<p>一个人工训练出来的模型是无法达到100%准确率的。因此我们需要一个东西来衡量模型训练的好坏。<strong>损失函数就是用来衡量模型的输出与真实输出的差别。</strong></p>
<p>下面就是逻辑回归的损失函数推导过程。</p>
假设只有两个标签1和0，<span
class="math inline">\(y_n\in\{0,1\}\)</span>。我们把采集到的任何一组样本看做一个事件的话，那么这个事件发生的概率假设为p。我们的模型y的值等于标签为1的概率也就是p，即：
<p>
<span
class="math display">\[P_{y=1}=\frac{1}{1+e^{-\omega^Tx}}=p\]</span>
</p>
<p>因为标签不是1就是0，因此标签为0的概率就是:</p>
<p>
<span class="math display">\[P_{y=0}=1-p\]</span>
</p>
我们把单个样本看做一个事件，那么这个事件发生的概率就是：
<p>
<span class="math display">\[P(y\ |\ x)=\begin{cases}p,&amp;y=1 \cr
1-p,&amp;y=0\end{cases}\]</span>
</p>
<p>这个函数不方便计算，它等价于:</p>
<p>
<span class="math display">\[P(y_i\ |\
x_i)=p^{y_i}(1-p)^{1-y_i}\]</span>
</p>
<p>解释下这个函数的含义，我们采集到了一个样本 <span
class="math inline">\((x_i,y_i)\)</span> 。对这个样本，它的标签是 <span
class="math inline">\(y_i\)</span> 的概率是 <span
class="math inline">\(p^{y_i}(1-p)^{1-{y_i}}\)</span>
。（当y=1，结果是p；当y=0，结果是1-p）。</p>
<p>如果我们采集到了一组数据一共N个， <span
class="math inline">\((x_1,y_1),(x_2,y_2),(x_3,y_3) \ldots
(x_N,y_N)\)</span>
，这个合成在一起的合事件发生的总概率怎么求呢？其实就是将每一个样本发生的概率相乘就可以了，即采集到这组样本的概率：</p>
<p>
<span class="math display">\[P_总=P(y_1|x_1)P(y_2|x_2)\cdots
P(y_n|x_n)=\prod_{n=1}^{N}p^{y_n}(1-p)^{(1-y_n)}\]</span>
</p>
<p><strong>注意 <span class="math inline">\(P_总\)</span>
是一个函数，并且未知的量只有 <span
class="math inline">\(\omega\)</span>（在p里面）。</strong></p>
由于连乘很复杂，我们通过两边取对数来把连乘变成连加的形式，即：
<p>
<span
class="math display">\[F(\omega)=ln(P_总)=ln(\prod_{n=1}^{N}p^{y_n}(1-p)^{(1-y_n)})\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad\quad\quad\quad\quad\quad=\sum_{n=1}^Nln(p^{y_n}(1-p)^{(1-y_n)})\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad=\sum_{n=1}^N(y_nln(p)+(1-y_n)ln(1-p))\]</span>
</p>
<p>其中，<span
class="math inline">\(p=\frac{1}{1+e^{-\omega^Tx}}\)</span></p>
<p>这个函数 <span class="math inline">\(F(\omega)\)</span>
就叫做它的<strong>损失函数</strong>。损失函数可以理解成衡量我们当前的模型的输出结果，跟实际的输出结果之间的差距的一种函数。这里的损失函数的值等于事件发生的总概率，我们希望它越大越好。但是跟损失的含义有点儿违背，因此也可以在前面取个负号，<strong>最终的表达式</strong>如下所示（1/m表示均值化）：</p>
<p>
<span
class="math display">\[F(\omega)=-\frac{1}{m}\sum_{n=1}^m(y_nln(p)+(1-y_n)ln(1-p))\]</span>
</p>
<h3
id="五极大似然估计mlemaximum-likelihood-estimation">五、极大似然估计MLE(Maximum
Likelihood Estimation)</h3>
<p>我们在真实世界中并不能直接看到概率是多少，我们只能观测到事件是否发生。也就是说，我们只能知道一个样本它实际的标签是1还是0。那么我们如何估计参数
<span class="math inline">\(\omega\)</span> 跟 <span
class="math inline">\(b\)</span> 的值呢？</p>
<p><strong>极大似然估计MLE</strong>(Maximum Likelihood
Estimation)，就是一种估计参数 <span
class="math inline">\(\omega\)</span> 的方法。在这里如何使用MLE来估计
<span class="math inline">\(\omega\)</span> 呢？</p>
<p>我们知道损失函数 <span class="math inline">\(F(\omega)\)</span>
是正比于总概率 <span class="math inline">\(P_总\)</span> 的，而 <span
class="math inline">\(F(\omega)\)</span> 又只有一个变量 <span
class="math inline">\(\omega\)</span> 。也就是说，通过改变 <span
class="math inline">\(\omega\)</span> 的值，就能得到不同的总概率值 <span
class="math inline">\(P_总\)</span> 。那么当我们选取的某个 <span
class="math inline">\(\omega^{\*}\)</span> 刚好使得总概率 <span
class="math inline">\(P_总\)</span> 取得最大值的时候。我们就认为这个
<span class="math inline">\(\omega^{\*}\)</span> 就是我们要求得的 <span
class="math inline">\(\omega\)</span>
的值，这就是极大似然估计的思想。</p>
<p><strong>现在我们的问题变成了，找到一个 <span
class="math inline">\(\omega^*\)</span>
，使得我们的总事件发生的概率，即损失函数 <span
class="math inline">\(F(\omega)\)</span>
取得最大值</strong>，这句话用数学语言表达就是：</p>
<p>
<span
class="math display">\[\omega^*=arg\mathop{max}\limits_{\omega}F(\omega)=-arg\mathop{min}\limits_{\omega}F(\omega)\]</span>
</p>
<h3 id="六梯度下降法">六、梯度下降法</h3>
<p>由于极大似然函数无法直接求解，所以在机器学习算法中，在最小化损失函数时，可以通过<strong>梯度下降法</strong>来一步步的迭代求解，得到最小化的损失函数和模型参数值。
#### 6.1 梯度 <strong>在微积分里面，对多元函数的参数求 <span
class="math inline">\(\delta\)</span>
偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。</strong>
比如函数 <span class="math inline">\(f(x,y)\)</span> , 分别对 <span
class="math inline">\(x,y\)</span> 求偏导数，求得的梯度向量就是 <span
class="math inline">\((\delta f/\delta x,\delta f/\delta
y)^T\)</span>，简称 <span class="math inline">\(grad f(x,y)\)</span>
或者 <span class="math inline">\(\nabla f(x,y)\)</span>。对于在点 <span
class="math inline">\(x_0,y_0\)</span> 的具体梯度向量就是 <span
class="math inline">\((\delta f/\delta x_0,\delta f/\delta
y_0)^T\)</span> 或者 <span class="math inline">\(\nabla
f(x_0,y_0)\)</span>。</p>
<p>那么这个梯度向量求出来有什么意义呢？<strong>从几何意义上讲，就是函数变化增加最快的地方</strong>。具体来说，对于函数
<span class="math inline">\(f(x,y)\)</span> 在点 <span
class="math inline">\(x_0,y_0\)</span> 沿着梯度方向的向量就是 <span
class="math inline">\(f(x,y)\)</span>
增加最快的地方。或者说沿着梯度向量的方向，更加容易找到函数的最大值；反过来说，沿着梯度向量相反的方向，也就是
<span class="math inline">\(-(\delta f/\delta x_0,\delta f/\delta
y_0)^T\)</span> 的方向，梯度减少的最快，也就更容易找到函数的最小值。</p>
<h4 id="梯度下降的直观解释">6.2 梯度下降的直观解释</h4>
<p>首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</p>
<p>从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<p><img src="https://s2.ax1x.com/2019/04/22/EkLNb4.png" /></p>
<h4 id="梯度下降法描述">6.3 梯度下降法描述</h4>
<h5 id="先决条件">6.3.1 先决条件</h5>
确认优化模型的假设函数和损失函数。 ##### 6.3.2 算法相关参数初始化
主要是初始化 <span
class="math inline">\(\omega_0,\omega_1\cdots,\omega_n\)</span>，算法终止距离
<span class="math inline">\(\varepsilon\)</span> 和步长 <span
class="math inline">\(\alpha\)</span>。在没有任何先验知识的时候，我们比较倾向于将所有的
<span class="math inline">\(\omega_i\)</span>
初始化为0，将步长初始化为1，在调优的时候再进行优化。 ##### 6.3.3
算法过程 (1) 确定当前位置的损失函数的梯度，对于 <span
class="math inline">\(\omega_i\)</span> ,其梯度表达式如下：
<p>
<span
class="math display">\[\frac{\delta}{\delta\omega_i}J(\omega_0,\omega_1,\cdots,\omega_n)\]</span>
</p>
<ol start="2" type="1">
<li>用步长乘以损失函数的梯度，得到当前位置下降的距离，即 <span
class="math inline">\(\alpha\frac{\delta}{\delta\omega_i}J(\omega_0,\omega_1,\cdots,\omega_n)\)</span>,对应于前面登山过程中的某一步。</li>
<li>确定是否所有的 <span class="math inline">\(\omega_i\)</span>
梯度下降的距离都小于 <span
class="math inline">\(\varepsilon\)</span>，如果小于 <span
class="math inline">\(\varepsilon\)</span> 则算法终止，当前所有的 <span
class="math inline">\(\omega_i\)</span>
即为最终结果，否则进入步骤4。</li>
<li>更新所有的 <span class="math inline">\(\omega_i\)</span>，对于 <span
class="math inline">\(\omega_i\)</span>，其更新表达式如下，更新完毕后继续转入步骤1。</li>
</ol>
<p>
<span
class="math display">\[\omega_i=\omega_i-\alpha\frac{\delta}{\delta\omega_i}J(\omega_0,\omega_1,\cdots,\omega_n)\]</span>
</p>
<h3 id="七求-fomega-的梯度-nabla-fomega">七、求 <span
class="math inline">\(F(\omega)\)</span> 的梯度 <span
class="math inline">\(\nabla F(\omega)\)</span></h3>
先回顾一下p的公式：
<p>
<span class="math display">\[p=\frac{1}{1+e^{-\omega^Tx}}\]</span>
</p>
<p>再回顾一下 <span class="math inline">\(F(\omega)\)</span>
的公式：</p>
<p>
<span
class="math display">\[F(\omega)=-\frac{1}{m}\sum_{n=1}^m(y_nln(p)+(1-y_n)ln(1-p))\]</span>
</p>
<span class="math inline">\(p\)</span> 是一个关于变量 <span
class="math inline">\(\omega\)</span> 的函数，我们对 <span
class="math inline">\(p\)</span>
求导，通过链式求导法则，慢慢展开可以得：
<p>
<span
class="math display">\[p^{&#39;}=(\frac{1}{1+e^{-\omega^Tx}})^{&#39;}\]</span>
</p>
<p>
<span class="math display">\[\qquad\qquad\quad\quad\qquad
=-\frac{1}{(1+e^{-\omega^Tx})^{2}}\cdot(1+e^{-\omega^Tx})^{&#39;}\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad=-\frac{1}{(1+e^{-\omega^Tx})^{2}}\cdot
e^{-\omega^Tx}\cdot (-\omega^Tx)^{&#39;}\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad\quad\quad\quad\quad\quad\quad=-\frac{1}{(1+e^{-\omega^Tx})^{2}}\cdot
e^{-\omega^Tx}\cdot (-x)\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad\quad\quad\quad\quad=\frac{1}{1+e^{-\omega^Tx}}\cdot
\frac{e^{-\omega^Tx}}{1+e^{-\omega^Tx}}\cdot x\]</span>
</p>
<p>
<span class="math display">\[=p(1-p)x\]</span>
</p>
上面都是我们做的准备工作，总之我们得记住：
<p>
<span class="math display">\[p^{&#39;}=p(1-p)x\]</span>
</p>
<p>
<span class="math display">\[(1-p)^{&#39;}=-p(1-p)x\]</span>
</p>
<p>下面我们正式开始对 <span class="math inline">\(F(\omega)\)</span>
求导，求导的时候请始终记住，我们的变量只有 <span
class="math inline">\(\omega\)</span>，其他的什么 <span
class="math inline">\(y_n,x_n\)</span> 都是已知的，可以看做常数。</p>
<p>
<span class="math display">\[\nabla
F(\omega)=-\frac{1}{m}\nabla(\sum_{n=1}^m(y_nln(p)+(1-y_n)ln(1-p)))\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad=-\frac{1}{m}\sum_{n=1}^m(y_nln^{&#39;}(p)+(1-y_n)ln^{&#39;}(1-p))\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad\quad\quad=-\frac{1}{m}\sum_{n=1}^m((y_n\frac{1}{p}p^{&#39;})+(1-y_n)\frac{1}{1-p}(1-p)^{&#39;})\]</span>
</p>
<p>
<span
class="math display">\[\quad=-\frac{1}{m}\sum_{n=1}^m(y_n(1-p)x_n-(1-y_n)px_n)\]</span>
</p>
<p>
<span
class="math display">\[=-\frac{1}{m}\sum_{n=1}^m(y_n-p)x_n\]</span>
</p>
终于，我们求出了梯度 <span class="math inline">\(\nabla
F(\omega)\)</span> 的表达式了，现在我们再来看看它长什么样子：
<p>
<span class="math display">\[ \nabla
F(\omega)=-\frac{1}{m}\sum_{n=1}^N(y_n-p)x_n\]</span>
</p>
<p>它是如此简洁优雅，这就是我们选取sigmoid函数的原因之一。当然我们也能够把
<span class="math inline">\(p\)</span> 再展开，即：</p>
<p>
<span class="math display">\[ \nabla
F(\omega)=-\frac{1}{m}\sum_{n=1}^N(y_n-\frac{1}{1+e^{-\omega^Tx_n}})x_n\]</span>
</p>
<h3 id="八更新系数-omega_i">八、更新系数 <span
class="math inline">\(\omega_i\)</span></h3>
<p>梯度下降法共分为<strong>批量梯度下降（Batch Gradient
Descent）、随机梯度下降（Stochastic Gradient
Descent）和小批量梯度下降（Mini-Batch Gradient Descent）</strong>。</p>
<p>具体知识请参考我的另一篇博客：</p>
<blockquote>
<p><a
href="http://qzmvc1.top/2019/04/23/%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-BGD-%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-SGD-%E3%80%81%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-MBGD/">批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD)</a></p>
</blockquote>
这里给出<strong>基于BGD梯度下降的公式</strong>：
<p>
<span class="math display">\[\omega_{i+1}=\omega_{i}-\alpha\nabla
F(\omega)\]</span>
</p>
<p>
<span
class="math display">\[\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad=\omega_{i}+\alpha\frac{1}{m}\sum_{n=1}^m(y_n-\frac{1}{1+e^{-\omega^Tx_n}})x_n\]</span>
</p>
<h3 id="九逻辑回归与正则化">九、逻辑回归与正则化</h3>
<p>有关L1正则化和L2正则化可以参考我的另一篇博客：</p>
<blockquote>
<p><a
href="http://qzmvc1.top/2019/04/23/%E6%B5%85%E8%B0%88L1%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8EL2%E6%AD%A3%E5%88%99%E5%8C%96/">浅谈L1正则化与L2正则化</a></p>
</blockquote>
<p>这里给出<strong>基于L2正则化的公式</strong>：</p>
<p>
<span
class="math display">\[F(\omega)=-\frac{1}{m}\sum_{n=1}^m(y_nln(p)+(1-y_n)ln(1-p))+\frac{\lambda}{2m}\sum_{j=1}^{k}\omega_j^2\]</span>
</p>
<h5 id="剩下的就是具体的敲代码了">剩下的就是具体的敲代码了！</h5>
<hr />
<h5 id="参考链接">参考链接:</h5>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44591359">逻辑回归 logistics
regression 公式推导</a></p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>觉得本文对你有用的话，不妨打个赏吧~</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="QzmVc1 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="QzmVc1 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>QzmVc1
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://qzmvc1.top/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0-%E4%B8%80-%E4%BB%8E%E9%9B%B6%E8%AE%A4%E8%AF%86%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.html" title="机器学习日记(一):从零认识逻辑回归">http://qzmvc1.top/机器学习日记-一-从零认识逻辑回归.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"><i class="fa fa-tag"></i> 算法</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B8%B8%E8%A7%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%AE%80%E4%BB%8B.html" rel="prev" title="神经网络常见激活函数简介">
                  <i class="fa fa-chevron-left"></i> 神经网络常见激活函数简介
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0-%E4%BA%8C-%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-BGD-%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-SGD-%E3%80%81%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-MBGD.html" rel="next" title="机器学习日记(二):批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD)">
                  机器学习日记(二):批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







  <div class="comments" id="comments">
    <script src="https://utteranc.es/client.js"
        repo="QzmVc1/blog-comment"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>
    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QzmVc1</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">456k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:55</span>
  </span>
</div>


<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>



<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>-->



    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




  <script async src="/js/cursor/fireworks.js"></script>

  
 
	<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
 

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>

